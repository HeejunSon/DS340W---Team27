# -*- coding: utf-8 -*-
"""DS340W Implementation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cNz4d3U2ZydULwVyxCtPw1zKF4m_I1fd

# Import libraries + Read data
"""

# Commented out IPython magic to ensure Python compatibility.
# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
from scipy.sparse import vstack
import gc
import tensorflow as tf
from tqdm import tqdm
import re
import seaborn as sns
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix
from sklearn.metrics import jaccard_score

# Read data
from google.colab import drive
drive.mount("/content/drive/")
# %cd '/content/drive/MyDrive/DS340/Netflix'
!pwd
Movie_Title = pd.read_csv("movie_titles.csv", encoding = 'ISO-8859-1', header = None, names = ['Id', 'Year', 'Name']).set_index('Id')
data1 = pd.read_csv("combined_data_1.txt", header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])
# data2 = pd.read_csv("combined_data_2.txt", header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])
# data3 = pd.read_csv("combined_data_3.txt", header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])
data4 = pd.read_csv("combined_data_4.txt", header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])

# filter out unncessary warnings
import warnings
warnings.filterwarnings('ignore')
# remove unnecessary TF logs
import logging
import tensorflow as tf
tf.get_logger().setLevel(logging.ERROR)

"""# Merging Dataset"""

# Random Sampling
#data1 = data1.sample(frac=0.01,replace=False)
#data2 = data2.sample(frac=0.01,replace=False)
#data3 = data3.sample(frac=0.01,replace=False)
#data4 = data4.sample(frac=0.01,replace=False)
# Merge 4 dataset
df = pd.concat([data1, data4]).reset_index(drop=True)
del data1, data4
_=gc.collect()

Movie_Title

"""# Below Code is from Kaggle

Reference: https://www.kaggle.com/code/fanglidayan/10-netflix-movie-recommender-part-1 
"""

# Find empty rows to slice dataframe for each movie
tmp_movies = df[df['Rating'].isna()]['User'].reset_index()
movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]
# Shift the movie_indices by one to get start and endpoints of all movies
shifted_movie_indices = deque(movie_indices)
shifted_movie_indices.rotate(-1)
# Gather all dataframes
user_data = []
# Iterate over all movies
for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):
  if df_id_1<df_id_2:
    tmp_df = df.loc[df_id_1+1:df_id_2-1].copy()
  else:
    tmp_df = df.loc[df_id_1+1:].copy()
  
  tmp_df['Movie'] = movie_id
  user_data.append(tmp_df)

# Combine all dataframes
df_cleaned = pd.concat(user_data)
del user_data, df, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id
df_cleaned.head()

"""# Novel Work - Heejun, So-I"""

df_cleaned.drop(columns=['Date'],inplace=True)

df_cleaned

Num_Rating = pd.DataFrame(df_cleaned.groupby('Movie')['Rating'].count())
Num_Rating = Num_Rating.rename(columns={"Rating": "Total No. Ratings"})
Avg_Rating = pd.DataFrame(df_cleaned.groupby('Movie')['Rating'].mean())
Avg_Rating = Avg_Rating.rename(columns={"Rating": "Avg. Ratings"})
Num_Rating

Avg_Rating

"""# Distribution of Average Ratings"""

Avg_Rating.hist()

"""# Distribution of Total Number of Ratings"""

Num_Rating[Num_Rating['Total No. Ratings'] < 10000].hist()

Num_Rating[Num_Rating['Total No. Ratings'] < 2000].hist()

Num_Rating.sort_values(by='Total No. Ratings', ascending= True)

"""# To see statistics"""

a = Num_Rating.describe()
b = Avg_Rating.describe()
Summary = pd.merge(a,b,left_index=True, right_index=True)
Summary

"""# Filter too few numbers of ratings for more accurate recommendation systems!

Threshold = 500
"""

# Filter out movies that have less than 500 reviews
Num_Rating = Num_Rating[Num_Rating['Total No. Ratings'] > 500].sort_values(by='Total No. Ratings', ascending=True)
Num_Rating

# Merge filtered Num_Rating dataframe with Avg_Rating
Joined = Num_Rating.join(Avg_Rating, on='Movie', how='inner')
Joined

"""# Use external data to add Genre variable

"""

Genre = pd.read_csv("movies_metadata.csv")

for i, title1 in tqdm(enumerate(Movie_Title['Name'].values)):
  for j, title2 in enumerate(Genre['original_title'].values):
    if title1 == title2:
      Movie_Title.at[i, 'genre'] = Genre['genres'][j]

Movie_Title = Movie_Title.dropna()
Movie_Title

"""# Export only Genre's id"""

import re
Movie_Title['genre'] = Movie_Title['genre'].map(lambda x: re.findall(r'\d+', x))
Movie_Title = Movie_Title.rename_axis('Movie')
Movie_Title

Movie_Title = Movie_Title.rename_axis('Movie')
Movie_Title

"""# Below Code shows why we need to use data1 and data4

- To have more movies with genres
- We can only use 2 out of 4 datasets due to memory capacity 
"""

print("MovieID between 1 and 4442: ", np.count_nonzero(Movie_Title.index < 4443))
print("MovieID between 4443 and 8884: ", np.count_nonzero(Movie_Title.index < 8885) - np.count_nonzero(Movie_Title.index < 4443))
print("MovieID between 8885 and 13326: ", np.count_nonzero(Movie_Title.index < 13327) - np.count_nonzero(Movie_Title.index < 8885))
print("MovieID between 13327 and 17769: ", np.count_nonzero(Movie_Title.index < 17770) - np.count_nonzero(Movie_Title.index < 13327))

"""# Popularity Approach

- Recommend movies that have the most number of ratings

- Recommend movies that have the highest average ratings
"""

Top_Num = Joined.sort_values(by='Total No. Ratings', ascending=False)
Popularity_Num = pd.merge(Top_Num, Movie_Title, left_index=True, right_index=True)
Popularity_Num.head(20)

Top_Avg = Joined.sort_values(by='Avg. Ratings', ascending=False)
Popularity_Rating = pd.merge(Top_Avg, Movie_Title, left_index=True, right_index=True)
Popularity_Rating.head(20)

"""# Content Based Filtering

- Use characteristics of movies to recommend
"""

# Avg. Rating by Movie ID
By_Movie = pd.DataFrame(df_cleaned.groupby('Movie')['Rating'].mean()).sort_values(by='Rating', ascending=False)
CB = pd.merge(By_Movie, Movie_Title, left_index=True, right_index=True)
CB

# To use overview feature from metadata.csv
MetaData = pd.read_csv("movies_metadata.csv")[['original_title', 'overview', 'vote_count']].dropna()
MetaData = MetaData.rename(columns={"original_title": "Name"})
MetaData = MetaData.sort_values(by='vote_count', ascending=False)
MetaData

# To check is there redundant movies for the same movie
MetaData_Dirt = MetaData.groupby('Name').count().sort_values(by='overview', ascending=False)
MetaData_Dirt

# To remove redundant movies for the same movie
movies_with_multiple_overviews = MetaData_Dirt[MetaData_Dirt['overview'] > 1].index
for i in range(len(movies_with_multiple_overviews)):
  k = MetaData[MetaData['Name'] == movies_with_multiple_overviews[i]]
  MetaData.drop(k.index[1:],inplace=True)
MetaData

MetaData.nunique()

MetaData.groupby('overview').count().sort_values(by='Name', ascending=False)

"""# Found that some movies do not have appropriate overview data

### Need to remove them!
"""

Remove_word1 = ['No', 'overview']
Remove_word2 = ['No', 'Overview']
Test1 = MetaData[MetaData['overview'].map(lambda x: all(string in x for string in Remove_word1))]
Test2 = MetaData[MetaData['overview'].map(lambda x: all(string in x for string in Remove_word2))]
Test = pd.concat([Test1, Test2])
Test

movies_with_no_overview = Test['Name'].values
for i in range(len(movies_with_no_overview)):
  L = MetaData[MetaData['Name'] == movies_with_no_overview[i]]
  MetaData.drop(L.index[0],inplace=True)
MetaData

MetaData[MetaData['vote_count'] < 50].hist()

CB = pd.merge(CB, MetaData, how='inner', on='Name')
CB.drop(columns=['vote_count'])

# Need to remove redundant movies
CB_Dirt = CB.groupby('Name').count().sort_values(by='overview', ascending=False)
Redundant = CB_Dirt[CB_Dirt['overview'] > 1].index
for i in range(len(Redundant)):
  k = CB[CB['Name'] == Redundant[i]]
  CB.drop(k.index[1:])
CB

"""# Below code is from Kaggle

Reference: https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system/notebook 
"""

#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

#Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix = tfidf.fit_transform(CB['overview'])

#Output the shape of tfidf_matrix
tfidf_matrix.shape

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
indices = pd.Series(CB.index, index=CB['Name']).drop_duplicates()
def get_recommendations(Name, cosine_sim=cosine_sim):
  idx = indices[Name]

  # Get the pairwsie similarity scores of all movies with that movie
  sim_scores = list(enumerate(cosine_sim[idx]))

  # Sort the movies based on the similarity scores
  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

  # Get the scores of the 20 most similar movies
  sim_scores = sim_scores[1:21]

  # Get the movie indices
  movie_indices = [i[0] for i in sim_scores]
  return CB['Name'].iloc[movie_indices]

# To check the function works appropriately
get_recommendations('House')

"""# Novel Work: 
# Content-Based Filtering by genre of movies
"""

# Define Jaccard Similarity
def jaccard(a, b):
    intersect = len(list(set(a).intersection(b)))
    union = (len(a) + len(b)) - intersect
    return float(intersect) / union
# Make Recommendation System
def CB_Recommender(Name):
  Jac_Sim = pd.DataFrame(columns = ['Name', 'Jac_Sim'])
  for i in range(len(CB['genre'])):
    good = CB[CB['Name'] == Name]['genre'][CB[CB['Name'] == Name]['genre'].index[0]]
    score = jaccard(good, CB['genre'][i])
    titles = CB['Name'][i]
    Jac_Sim = Jac_Sim.append({'Name':titles,'Jac_Sim':score}, ignore_index=True)
    Jac_Sim = Jac_Sim.sort_values(by='Jac_Sim', ascending= False)
    Jac_Sim = Jac_Sim[Jac_Sim['Jac_Sim'] < 1.0]
  return Jac_Sim.head(20)

CB_Recommender('House')

"""# Collaborative Filtering
- Use users' information history for recommendation systems

# Since dataset is too large, we decided to filter top n users who reviewd the movies most.
- n = 100
"""

Top_Users = pd.DataFrame(df_cleaned.groupby('User')['Rating'].count().sort_values(ascending= False))
Top_Users = Top_Users.head(100)
Top_Users

CF_base = pd.DataFrame(columns = ['User', 'Rating', 'Movie'])
for i in range(len(Top_Users.index)):
  a = df_cleaned[df_cleaned['User'] == Top_Users.index[i]]
  CF_base = CF_base.append(a, ignore_index=True)
CF_base

"""# Modulde "Surprise" is introduced by **The Journal of Open Source Software**"

- Reference: Hug, N., (2020). Surprise: A Python library for recommender systems. Journal of Open Source Software, 5(52), 2174. https://doi.org/10.
21105/joss.02174
"""

pip install scikit-surprise

"""# Below code based on Kaggle code!

- We built our code on top of Kaggle code!

- Reference: https://www.kaggle.com/code/jieyima/netflix-recommendation-collaborative-filtering/notebook 
"""

from surprise import Reader, Dataset, SVD
# from surprise.model_selection import cross_validate
reader = Reader()
svd = SVD()
def CF_Recommender(User):
  CF_Title = Movie_Title[['Year', 'Name']].reset_index()
  # getting full dataset
  data = Dataset.load_from_df(CF_base[['User', 'Movie', 'Rating']], reader)
  trainset = data.build_full_trainset()
  svd.fit(trainset)
  CF_Title['Score'] = CF_Title['Movie'].apply(lambda x: svd.predict(User, x).est)
  CF_Title = CF_Title[['Year', 'Name', 'Score']]
  CF_Title = CF_Title.sort_values('Score', ascending=False)
  return(CF_Title.head(20))

CF_Recommender(305344)

"""# Hybrid Approach

- Combine Content-Based Filtering with Collaborative Filtering
"""

